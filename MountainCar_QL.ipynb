{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# エージェントを動かす\n",
    "0: 左へおす\\\n",
    "1: 何もしない\\\n",
    "2: 右へおす\n",
    "```\n",
    "action = 0~2\n",
    "env.step(action)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "import numpy as np\n",
    "env = gym.make('MountainCar-v0')\n",
    "env = wrappers.Monitor(env, './gym-results/QL', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.53406627  0.        ]\n"
     ]
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "print(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99 #時間割引率\n",
    "ALPHA = 0.2 #learning rate\n",
    "DIVISOR = 40 #状態を離散値に振り分けるときの領域の数\n",
    "#Q-Learningの処理\n",
    "class QL_Brain:\n",
    "    def __init__(self, env , num_states, num_actions):\n",
    "        self.env = env\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions \n",
    "        #q table 車の位置x車の速度x行動\n",
    "        self.q_table = np.zeros((40,40,3))\n",
    "    \n",
    "    def get_status(self, observation):\n",
    "        '''車の位置，速度は連続値→離散値に変換'''\n",
    "        env_low = self.env.observation_space.low\n",
    "        env_high = self.env.observation_space.high\n",
    "        env_dx = (env_high - env_low) / 40\n",
    "        position = int((observation[0] - env_low[0]) / env_dx[0])\n",
    "        velocity = int((observation[1] - env_low[1]) / env_dx[1])\n",
    "        return position, velocity\n",
    "\n",
    "    def update_q_table(self, action, observation, next_observation, reward):\n",
    "        '''q tableの更新する'''\n",
    "        #行動後の状態で得られる最大行動価値\n",
    "        next_position, next_velocity = self.get_status(next_observation) \n",
    "        next_max_q_value = max(self.q_table[next_position][next_velocity])\n",
    "        #行動前の状態の行動価値\n",
    "        position, velocity = self.get_status(observation)\n",
    "        q_value = self.q_table[position][velocity][action]\n",
    "        #行動価値関数の更新\n",
    "        self.q_table[position][velocity][action] = q_value + ALPHA * (reward + GAMMA * next_max_q_value - q_value)\n",
    "\n",
    "        return self.q_table\n",
    "    \n",
    "    def decide_action(self, observation):\n",
    "        '''ε-greedy法を用いて，行動を決定'''\n",
    "        epsilon = 0.002\n",
    "        if np.random.uniform(0,1) > epsilon: #99.8%の確率で最大の行動価値の行動を選択\n",
    "            position, velocity = self.get_status(observation)\n",
    "            action = np.argmax(self.q_table[position][velocity])\n",
    "        else:\n",
    "            action = np.random.choice([0,1,2]) #0.2%の確率で行動をrandomchoice\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent\n",
    "class Agent:\n",
    "    def __init__(self, env, num_states, num_actions):\n",
    "        self.brain = QL_Brain(env, num_states, num_actions)\n",
    "    \n",
    "    def update_q_function(self, action, observation, next_observation, reward):\n",
    "        '''Q関数を更新する'''\n",
    "        self.brain.update_q_table(action, observation, next_observation, reward)\n",
    "    \n",
    "    def get_action(self, observation):\n",
    "        '''行動を決定する'''\n",
    "        action = self.brain.decide_action(observation)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPISODES = 10000 #試行回数\n",
    "MAX_STEPS = 200 #1試行のステップ数\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make('MountainCar-v0')\n",
    "        self.env = wrappers.Monitor(env, './gym-results/QL', force=True) #動画保存\n",
    "        self.num_states = self.env.observation_space.shape[0]\n",
    "        self.num_actions = self.env.action_space.n\n",
    "        self.rewards = []\n",
    "        self.agent = Agent(self.env, self.num_states, self.num_actions)\n",
    "    \n",
    "    def run(self): \n",
    "        '''実行'''\n",
    "        observation = self.env.reset() #環境の初期化\n",
    "        for episode in range(NUM_EPISODES):\n",
    "            total_reward = 0\n",
    "            observation = self.env.reset() #環境の初期化\n",
    "            for _ in range(MAX_STEPS): #1episodeのループ\n",
    "                #ε-greedy法で行動選択\n",
    "                action = self.agent.get_action(observation)\n",
    "                #選択した行動で車をうごかし，その後の観測結果，報酬，到達フラグを取得\n",
    "                next_observation, reward, done, _ = self.env.step(action)\n",
    "                #infoはない, rewardはMountainCarではステップごとに固定で-1\n",
    "                \n",
    "                #Q tableの更新\n",
    "                self.agent.update_q_function(action, observation, next_observation, reward)\n",
    "                #報酬を与える\n",
    "                total_reward += reward\n",
    "                #観測の更新\n",
    "                observation = next_observation\n",
    "                if done: #車が頂上に到達したら\n",
    "                    #1episode終了\n",
    "                    if episode%100 == 0:\n",
    "                        print('episode: {}, total_reward: {}'.format(episode, total_reward))\n",
    "                    self.rewards.append(total_reward)\n",
    "                    #頂上に到達したので，1episode終了\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0, total_reward: -200.0\n",
      "episode: 100, total_reward: -200.0\n",
      "episode: 200, total_reward: -200.0\n",
      "episode: 300, total_reward: -200.0\n",
      "episode: 400, total_reward: -200.0\n",
      "episode: 500, total_reward: -200.0\n",
      "episode: 600, total_reward: -200.0\n",
      "episode: 700, total_reward: -200.0\n",
      "episode: 800, total_reward: -200.0\n",
      "episode: 900, total_reward: -200.0\n",
      "episode: 1000, total_reward: -200.0\n",
      "episode: 1100, total_reward: -200.0\n",
      "episode: 1200, total_reward: -200.0\n",
      "episode: 1300, total_reward: -200.0\n",
      "episode: 1400, total_reward: -200.0\n",
      "episode: 1500, total_reward: -200.0\n",
      "episode: 1600, total_reward: -200.0\n",
      "episode: 1700, total_reward: -192.0\n",
      "episode: 1800, total_reward: -200.0\n",
      "episode: 1900, total_reward: -200.0\n",
      "episode: 2000, total_reward: -200.0\n",
      "episode: 2100, total_reward: -200.0\n",
      "episode: 2200, total_reward: -200.0\n",
      "episode: 2300, total_reward: -200.0\n",
      "episode: 2400, total_reward: -200.0\n",
      "episode: 2500, total_reward: -200.0\n",
      "episode: 2600, total_reward: -199.0\n",
      "episode: 2700, total_reward: -198.0\n",
      "episode: 2800, total_reward: -167.0\n",
      "episode: 2900, total_reward: -200.0\n",
      "episode: 3000, total_reward: -162.0\n",
      "episode: 3100, total_reward: -200.0\n",
      "episode: 3200, total_reward: -200.0\n",
      "episode: 3300, total_reward: -159.0\n",
      "episode: 3400, total_reward: -200.0\n",
      "episode: 3500, total_reward: -200.0\n",
      "episode: 3600, total_reward: -200.0\n",
      "episode: 3700, total_reward: -157.0\n",
      "episode: 3800, total_reward: -188.0\n",
      "episode: 3900, total_reward: -200.0\n",
      "episode: 4000, total_reward: -192.0\n",
      "episode: 4100, total_reward: -185.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-622931916689>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmountaincar_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmountaincar_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-dcc3594610b9>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0mnext_observation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0;31m#Q tableの更新\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_q_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_observation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m                 \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_observation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-4948a9494629>\u001b[0m in \u001b[0;36mupdate_q_function\u001b[0;34m(self, action, observation, next_observation, reward)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_q_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_observation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;34m'''Q関数を更新する'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_q_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_observation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-06bf449f3567>\u001b[0m in \u001b[0;36mupdate_q_table\u001b[0;34m(self, action, observation, next_observation, reward)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;34m'''q tableの更新する'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m#行動後の状態で得られる最大行動価値\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mnext_position\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_velocity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_observation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mnext_max_q_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_position\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_velocity\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m#行動前の状態の行動価値\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-06bf449f3567>\u001b[0m in \u001b[0;36mget_status\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0menv_low\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0menv_high\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0menv_dx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0menv_high\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0menv_low\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mposition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0menv_low\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0menv_dx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mvelocity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0menv_low\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0menv_dx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mountaincar_env = Environment()\n",
    "mountaincar_env.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title('Q-Learning: Reward Transition')\n",
    "x = list(range(0, NUM_EPISODES))\n",
    "plt.plot(x, mountaincar_env.rewards)\n",
    "plt.ylabel('total_rewards')\n",
    "plt.xlabel('episodes')\n",
    "plt.grid()\n",
    "plt.savefig('./fig/QL/QL_RT.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
